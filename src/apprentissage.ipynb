{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from functions.download_data import (\n",
    "    get_patchs_labels,\n",
    "    normalization_params,\n",
    "    get_golden_paths,\n",
    "    pooled_std_dev,\n",
    ")\n",
    "\n",
    "from functions.filter import filter_indices_from_labels\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from functions.instanciators import get_dataset, get_lightning_module, get_trainer\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch import Generator\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345 \n",
    "torch.manual_seed(seed)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# experiment_name = \"test-dev\"\n",
    "# run_name =  \"kikito_stagios\"\n",
    "# \n",
    "# \n",
    "# \n",
    "# augment_size = 512\n",
    "# \n",
    "# \n",
    "# logits = 1\n",
    "# freeze_encoder = 0\n",
    "# epochs = 10\n",
    "# \n",
    "# test_batch_size = 8\n",
    "# num_sanity_val_steps = 1\n",
    "# accumulate_batch = 8\n",
    "# module_name = \"segformer-b5\"\n",
    "# loss_name =  \"cross_entropy_weighted\"\n",
    "# building_class_weight = 1\n",
    "# label_smoothing = 0.0\n",
    "# lr = 0.00005\n",
    "# momentum = float\n",
    "# scheduler_name = \"one_cycle\"\n",
    "# scheduler_patience = 3\n",
    "# patience = 200\n",
    "# \n",
    "# \n",
    "# cuda = 0\n",
    "# cuda = cuda and torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train\n",
    "from_s3 = 0\n",
    "task = \"segmentation\"\n",
    "source = \"PLEIADES\"\n",
    "dep, year  = \"MARTINIQUE\", \"2022\"\n",
    "tiles_size = 250\n",
    "type_labeler = \"BDTOPO\"\n",
    "\n",
    "patches, labels = get_patchs_labels(\n",
    "        from_s3, task, source, dep, year, tiles_size, type_labeler, train=True\n",
    "    )\n",
    "\n",
    "train_patches = []\n",
    "train_labels = []\n",
    "test_patches = []\n",
    "test_labels = []\n",
    "normalization_means = []\n",
    "normalization_stds = []\n",
    "weights = []\n",
    "\n",
    "patches.sort()\n",
    "labels.sort()\n",
    "indices = filter_indices_from_labels(labels, -1.0, 2.0)\n",
    "train_patches += [patches[idx] for idx in indices]\n",
    "train_labels += [labels[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test\n",
    "patches, labels = get_patchs_labels(\n",
    "    from_s3, task, source, dep, year, tiles_size, type_labeler, train=False\n",
    ")\n",
    "\n",
    "patches.sort()\n",
    "labels.sort()\n",
    "test_patches += list(patches)\n",
    "test_labels += list(labels)\n",
    "\n",
    "# Normalisation\n",
    "normalization_mean, normalization_std = normalization_params(\n",
    "    task, source, dep, year, tiles_size, type_labeler\n",
    ")\n",
    "normalization_means.append(normalization_mean)\n",
    "normalization_stds.append(normalization_std)\n",
    "weights.append(len(indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0026.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0026.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0512_8592_U38S_8Bits_0005.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0512_8592_U38S_8Bits_0005.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0025.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0025.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0031.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0031.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0034.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0034.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8594_U38S_8Bits_0005.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8594_U38S_8Bits_0005.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0029.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0029.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0011.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0011.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8595_U38S_8Bits_0003.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8595_U38S_8Bits_0003.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0005.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0005.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0006.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0006.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0003.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0003.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0041.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0041.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0028.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0028.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0013.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0013.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0019.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0019.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0045.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0045.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0040.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0040.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0039.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0039.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0043.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0043.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0044.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0044.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0007.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0007.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0049.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0049.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0000.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0000.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0009.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0009.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0004.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0004.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0047.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0047.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0012.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0012.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0006.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0006.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0014.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0014.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0015.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0015.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0018.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0018.jp2`\n",
      "Total: 2.36 MiB, Transferred: 2.36 MiB, Speed: 7.67 MiB/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0026.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0026.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8595_U38S_8Bits_0003.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8595_U38S_8Bits_0003.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0025.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0025.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0011.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0011.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0029.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0029.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0005.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0005.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0006.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0006.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0031.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0031.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0003.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0003.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0044.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0044.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8594_U38S_8Bits_0005.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8594_U38S_8Bits_0005.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0039.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0039.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0034.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0034.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0512_8592_U38S_8Bits_0005.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0512_8592_U38S_8Bits_0005.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0040.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0040.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0047.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0047.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0019.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0019.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0013.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0013.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0006.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0006.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0000.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0000.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0041.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0041.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0004.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0004.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0009.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0009.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0014.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0014.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0012.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0012.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0018.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0018.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0028.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0028.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0043.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0043.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0049.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0049.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0045.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0045.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0007.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0007.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0015.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0015.npy`\n",
      "Total: 1.91 MiB, Transferred: 1.91 MiB, Speed: 6.36 MiB/s\n"
     ]
    }
   ],
   "source": [
    "# Golden test\n",
    "golden_patches, golden_labels = get_golden_paths(\n",
    "    from_s3, task, source, \"MAYOTTE_CLEAN\", \"2022\", tiles_size\n",
    ")\n",
    "\n",
    "golden_patches.sort()\n",
    "golden_labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "\n",
    "n_bands = 3\n",
    "normalization_mean = np.average(\n",
    "    [mean[:n_bands] for mean in normalization_means], weights=weights, axis=0\n",
    ")\n",
    "normalization_std = [\n",
    "    pooled_std_dev(\n",
    "        weights,\n",
    "        [mean[i] for mean in normalization_means],\n",
    "        [std[i] for std in normalization_stds],\n",
    "    )\n",
    "    for i in range(n_bands)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_list = [\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.Normalize(\n",
    "        max_pixel_value=1.0,\n",
    "        mean=normalization_mean,\n",
    "        std=normalization_std,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "]\n",
    "\n",
    "augment_size = 250\n",
    "if augment_size != tiles_size:\n",
    "    transform_list.insert(0, A.Resize(augment_size, augment_size))\n",
    "transform = A.Compose(transform_list)\n",
    "\n",
    "test_transform_list = [\n",
    "    A.Normalize(\n",
    "        max_pixel_value=1.0,\n",
    "        mean=normalization_mean,\n",
    "        std=normalization_std,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "]\n",
    "if augment_size != tiles_size:\n",
    "    test_transform_list.insert(0, A.Resize(augment_size, augment_size))\n",
    "test_transform = A.Compose(test_transform_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(task, train_patches, train_labels, n_bands, from_s3, transform)\n",
    "dataset = get_dataset(task, train_patches[:40], train_labels[:40], n_bands, from_s3, transform)\n",
    "test_dataset = get_dataset(task, test_patches, test_labels, n_bands, from_s3, test_transform)\n",
    "golden_dataset = get_dataset(\n",
    "    task, golden_patches, golden_labels, n_bands, from_s3, test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=Generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "test_batch_size = 8\n",
    "cuda = 0\n",
    "kwargs = {\"num_workers\": os.cpu_count(), \"pin_memory\": True} if cuda else {}\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=test_batch_size, shuffle=False, drop_last=True, **kwargs\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, shuffle=False, drop_last=True, **kwargs\n",
    ")\n",
    "golden_loader = DataLoader(\n",
    "    golden_dataset, batch_size=test_batch_size, shuffle=False, drop_last=True, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'deeplabv3': <class 'models.components.segmentation_models.DeepLabv3Module'>, 'single_class_deeplabv3': <class 'models.components.segmentation_models.SingleClassDeepLabv3Module'>, 'segformer-b0': <class 'models.components.segmentation_models.SegformerB0'>, 'segformer-b1': <class 'models.components.segmentation_models.SegformerB1'>, 'segformer-b2': <class 'models.components.segmentation_models.SegformerB2'>, 'segformer-b3': <class 'models.components.segmentation_models.SegformerB3'>, 'segformer-b4': <class 'models.components.segmentation_models.SegformerB4'>, 'segformer-b5': <class 'models.components.segmentation_models.SegformerB5'>, 'UNetGaetan': <class 'modeles_gaetan.UNet'>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.11/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from modeles_gaetan import UNet\n",
    "from config.module import module_dict\n",
    "from functions.instanciators import get_model\n",
    "\n",
    "print(module_dict)\n",
    "module_name = \"UNetGaetan\"\n",
    "\n",
    "model = get_model(module_name,3,True,False)\n",
    "\n",
    "# test data\n",
    "batch = next(iter(train_loader))\n",
    "labels = batch[\"labels\"]\n",
    "images = batch[\"pixel_values\"]\n",
    "\n",
    "output = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.instanciators import get_loss\n",
    "loss_name =  \"cross_entropy_weighted\"\n",
    "building_class_weight = 1\n",
    "label_smoothing = 0.0\n",
    "\n",
    "loss = get_loss(\n",
    "        loss_name, building_class_weight=building_class_weight, label_smoothing=label_smoothing\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "patience = 200\n",
    "earlystop = {\"monitor\": \"validation_loss\", \"patience\": patience, \"mode\": \"min\"}\n",
    "checkpoints = [\n",
    "    {\n",
    "        \"monitor\": \"validation_loss\",\n",
    "        \"save_top_k\": 1,\n",
    "        \"save_last\": False,\n",
    "        \"mode\": \"min\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "label_smoothing = 0.0\n",
    "epochs = 1\n",
    "num_sanity_val_steps = 1\n",
    "accumulate_batch = 8\n",
    "logits = 1\n",
    "freeze_encoder = 0\n",
    "lr = 0.00005\n",
    "momentum = float\n",
    "scheduler_name = \"one_cycle\"\n",
    "scheduler_patience = 3\n",
    "\n",
    "trainer = get_trainer(earlystop, checkpoints, epochs, num_sanity_val_steps, accumulate_batch)\n",
    "\n",
    "light_module = get_lightning_module(\n",
    "    module_name=module_name,\n",
    "    loss_name=loss_name,\n",
    "    building_class_weight=building_class_weight,\n",
    "    label_smoothing=label_smoothing,\n",
    "    n_bands=n_bands,\n",
    "    logits=bool(logits),\n",
    "    freeze_encoder=bool(freeze_encoder),\n",
    "    task=task,\n",
    "    lr=lr,\n",
    "    momentum=momentum,\n",
    "    earlystop=earlystop,\n",
    "    scheduler_name=scheduler_name,\n",
    "    scheduler_patience=scheduler_patience,\n",
    "    cuda=cuda,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/9', creation_time=1697531551081, experiment_id='9', last_update_time=1697531551081, lifecycle_stage='active', name='test-dev', tags={}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_server_uri = \"https://projet-slums-detection-128833.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"test-dev\"\n",
    "run_name =  \"kikito_stagios2\"\n",
    "\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/22 15:52:21 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/22 15:52:21 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/mamba/lib/python3.11/site-packages/mlflow/pytorch/_lightning_autolog.py:463: UserWarning: Autologging is known to be compatible with pytorch-lightning versions between 1.4.9 and 2.2.0.post0 and may not succeed with packages outside this range.\"\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/mamba/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=71` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | UNet             | 31.0 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "31.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.0 M    Total params\n",
      "124.127   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36839ecde55b4b75baf1bb885124c7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=71` in the `DataLoader` to improve performance.\n",
      "/opt/mamba/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f0ff9e242b4978b48887a38f337dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52d30842cfb4907bf28e304ec93d0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/22 15:52:35 WARNING mlflow.utils.checkpoint_utils: Checkpoint logging is skipped, because checkpoint 'save_best_only' config is True, it requires to compare the monitored metric value, but the provided monitored metric value is not available.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "(\"Failed to copy the specified code path 'src/models/' into the model artifacts. It appears that your code path includes file(s) that cannot be copied. Please specify a code path that does not include such files and try again.\",)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/mlflow/utils/model_utils.py:146\u001b[0m, in \u001b[0;36m_validate_and_copy_code_paths\u001b[0;34m(code_paths, path, default_subpath)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     _copy_file_or_tree(src\u001b[39m=\u001b[39;49mcode_path, dst\u001b[39m=\u001b[39;49mpath, dst_dir\u001b[39m=\u001b[39;49mcode_dir_subpath)\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     \u001b[39m# A common error is code-paths includes Databricks Notebook. We include it in error\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[39m# message when running in Databricks, but not in other envs tp avoid confusion.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/mlflow/utils/file_utils.py:581\u001b[0m, in \u001b[0;36m_copy_file_or_tree\u001b[0;34m(src, dst, dst_dir)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopytree(src\u001b[39m=\u001b[39;49msrc, dst\u001b[39m=\u001b[39;49mdst_path, ignore\u001b[39m=\u001b[39;49mshutil\u001b[39m.\u001b[39;49mignore_patterns(\u001b[39m\"\u001b[39;49m\u001b[39m__pycache__\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m dst_subpath\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/shutil.py:559\u001b[0m, in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    558\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mshutil.copytree\u001b[39m\u001b[39m\"\u001b[39m, src, dst)\n\u001b[0;32m--> 559\u001b[0m \u001b[39mwith\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(src) \u001b[39mas\u001b[39;00m itr:\n\u001b[1;32m    560\u001b[0m     entries \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(itr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'src/models/'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m best_model \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(light_module)\u001b[39m.\u001b[39mload_from_checkpoint(\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     checkpoint_path\u001b[39m=\u001b[39mtrainer\u001b[39m.\u001b[39mcheckpoint_callback\u001b[39m.\u001b[39mbest_model_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     model\u001b[39m=\u001b[39mlight_module\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     scheduler_interval\u001b[39m=\u001b[39mlight_module\u001b[39m.\u001b[39mscheduler_interval,\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Logging the model with the associated code\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m mlflow\u001b[39m.\u001b[39;49mpytorch\u001b[39m.\u001b[39;49mlog_model(\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     artifact_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     code_paths\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39msrc/models/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39msrc/optim/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39msrc/config/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     pytorch_model\u001b[39m=\u001b[39;49mbest_model\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Log normalization parameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m mlflow\u001b[39m.\u001b[39mlog_params(\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     {\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnormalization_mean\u001b[39m\u001b[39m\"\u001b[39m: normalization_mean\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnormalization_std\u001b[39m\u001b[39m\"\u001b[39m: normalization_std,\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell://projet-slums-detection-851459-0.user.lab.sspcloud.fr/home/onyxia/work/satellite-images-train/src/apprentissage.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/mlflow/pytorch/__init__.py:301\u001b[0m, in \u001b[0;36mlog_model\u001b[0;34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, requirements_file, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mLog a PyTorch model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39m    PyTorch logged models\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m pickle_module \u001b[39m=\u001b[39m pickle_module \u001b[39mor\u001b[39;00m mlflow_pytorch_pickle_module\n\u001b[0;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m Model\u001b[39m.\u001b[39;49mlog(\n\u001b[1;32m    302\u001b[0m     artifact_path\u001b[39m=\u001b[39;49martifact_path,\n\u001b[1;32m    303\u001b[0m     flavor\u001b[39m=\u001b[39;49mmlflow\u001b[39m.\u001b[39;49mpytorch,\n\u001b[1;32m    304\u001b[0m     pytorch_model\u001b[39m=\u001b[39;49mpytorch_model,\n\u001b[1;32m    305\u001b[0m     conda_env\u001b[39m=\u001b[39;49mconda_env,\n\u001b[1;32m    306\u001b[0m     code_paths\u001b[39m=\u001b[39;49mcode_paths,\n\u001b[1;32m    307\u001b[0m     pickle_module\u001b[39m=\u001b[39;49mpickle_module,\n\u001b[1;32m    308\u001b[0m     registered_model_name\u001b[39m=\u001b[39;49mregistered_model_name,\n\u001b[1;32m    309\u001b[0m     signature\u001b[39m=\u001b[39;49msignature,\n\u001b[1;32m    310\u001b[0m     input_example\u001b[39m=\u001b[39;49minput_example,\n\u001b[1;32m    311\u001b[0m     await_registration_for\u001b[39m=\u001b[39;49mawait_registration_for,\n\u001b[1;32m    312\u001b[0m     requirements_file\u001b[39m=\u001b[39;49mrequirements_file,\n\u001b[1;32m    313\u001b[0m     extra_files\u001b[39m=\u001b[39;49mextra_files,\n\u001b[1;32m    314\u001b[0m     pip_requirements\u001b[39m=\u001b[39;49mpip_requirements,\n\u001b[1;32m    315\u001b[0m     extra_pip_requirements\u001b[39m=\u001b[39;49mextra_pip_requirements,\n\u001b[1;32m    316\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    317\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    318\u001b[0m )\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/mlflow/models/model.py:619\u001b[0m, in \u001b[0;36mModel.log\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m     run_id \u001b[39m=\u001b[39m mlflow\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mfluent\u001b[39m.\u001b[39m_get_or_start_run()\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[1;32m    618\u001b[0m mlflow_model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(artifact_path\u001b[39m=\u001b[39martifact_path, run_id\u001b[39m=\u001b[39mrun_id, metadata\u001b[39m=\u001b[39mmetadata)\n\u001b[0;32m--> 619\u001b[0m flavor\u001b[39m.\u001b[39;49msave_model(path\u001b[39m=\u001b[39;49mlocal_path, mlflow_model\u001b[39m=\u001b[39;49mmlflow_model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    621\u001b[0m \u001b[39m# Copy model metadata files to a sub-directory 'metadata',\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39m# For UC sharing use-cases.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m metadata_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(local_path, \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/mlflow/pytorch/__init__.py:469\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(pytorch_model, path, conda_env, mlflow_model, code_paths, pickle_module, signature, input_example, requirements_file, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     mlflow_model\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m metadata\n\u001b[0;32m--> 469\u001b[0m code_dir_subpath \u001b[39m=\u001b[39m _validate_and_copy_code_paths(code_paths, path)\n\u001b[1;32m    471\u001b[0m model_data_subpath \u001b[39m=\u001b[39m _MODEL_DATA_SUBPATH\n\u001b[1;32m    472\u001b[0m model_data_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, model_data_subpath)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/mlflow/utils/model_utils.py:151\u001b[0m, in \u001b[0;36m_validate_and_copy_code_paths\u001b[0;34m(code_paths, path, default_subpath)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m             \u001b[39m# A common error is code-paths includes Databricks Notebook. We include it in error\u001b[39;00m\n\u001b[1;32m    149\u001b[0m             \u001b[39m# message when running in Databricks, but not in other envs tp avoid confusion.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m             example \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, such as Databricks Notebooks\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m is_in_databricks_runtime() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 151\u001b[0m             \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    152\u001b[0m                 message\u001b[39m=\u001b[39m(\n\u001b[1;32m    153\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to copy the specified code path \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcode_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m into the model \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39martifacts. It appears that your code path includes file(s) that cannot \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbe copied\u001b[39m\u001b[39m{\u001b[39;00mexample\u001b[39m}\u001b[39;00m\u001b[39m. Please specify a code path that does not include \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39msuch files and try again.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    157\u001b[0m                 ),\n\u001b[1;32m    158\u001b[0m                 error_code\u001b[39m=\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[1;32m    159\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     code_dir_subpath \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mMlflowException\u001b[0m: (\"Failed to copy the specified code path 'src/models/' into the model artifacts. It appears that your code path includes file(s) that cannot be copied. Please specify a code path that does not include such files and try again.\",)"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=run_name):\n",
    "    mlflow.pytorch.autolog()\n",
    "    # 7- Training the model on the training set\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    gc.collect()\n",
    "\n",
    "    trainer.fit(light_module, train_loader, val_loader)\n",
    "\n",
    "    best_model = type(light_module).load_from_checkpoint(\n",
    "        checkpoint_path=trainer.checkpoint_callback.best_model_path,\n",
    "        model=light_module.model,\n",
    "        loss=light_module.loss,\n",
    "        optimizer=light_module.optimizer,\n",
    "        optimizer_params=light_module.optimizer_params,\n",
    "        scheduler=light_module.scheduler,\n",
    "        scheduler_params=light_module.scheduler_params,\n",
    "        scheduler_interval=light_module.scheduler_interval,\n",
    "    )\n",
    "\n",
    "    # Logging the model with the associated code\n",
    "    mlflow.pytorch.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        code_paths=[\n",
    "            \"src/models/\",\n",
    "            \"src/optim/\",\n",
    "            \"src/config/\",\n",
    "        ],\n",
    "        pytorch_model=best_model.to(\"cpu\"),\n",
    "    )\n",
    "\n",
    "    # Log normalization parameters\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"normalization_mean\": normalization_mean.tolist(),\n",
    "            \"normalization_std\": normalization_std,\n",
    "        }\n",
    "    )\n",
    "    # TODO: Add signature for inference\n",
    "\n",
    "    # 8- Test\n",
    "    trainer.test(dataloaders=[test_loader, golden_loader], ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'labels', 'metadata'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for batch in  train_loader :\n",
    " #   print batch.keys()\n",
    "\n",
    "iterateur  = iter(train_loader)\n",
    "\n",
    "batch = next(iterateur)\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 250, 250])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_image = batch[\"pixel_values\"]\n",
    "batch_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 63, 63])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = best_model(batch_image)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"metadata\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
