{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from functions.download_data import (\n",
    "    get_patchs_labels,\n",
    "    normalization_params,\n",
    "    get_golden_paths,\n",
    "    pooled_std_dev,\n",
    ")\n",
    "\n",
    "from functions.filter import filter_indices_from_labels\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from functions.instanciators import get_dataset, get_lightning_module, get_trainer\n",
    "from functions.instanciators import get_model\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch import Generator\n",
    "from config.module import module_dict\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import gc\n",
    "\n",
    "seed = 12345 \n",
    "torch.manual_seed(seed)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "#from torchvision.models.resnet import resnet50\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_server_uri = \"https://projet-slums-detection-128833.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"test-dev\"\n",
    "run_name =  \"stagiosessaye\"\n",
    "task = \"segmentation\"\n",
    "source = \"PLEIADES\"\n",
    "deps =  [\"MARTINIQUE\"]\n",
    "years = [\"2022\"]\n",
    "tiles_size = 250\n",
    "augment_size = 250\n",
    "type_labeler = \"BDTOPO\"\n",
    "n_bands = 3\n",
    "logits = 1\n",
    "freeze_encoder = 0\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "test_batch_size = 8\n",
    "num_sanity_val_steps = 1\n",
    "accumulate_batch = 8\n",
    "module_name = \"PSPNet\"\n",
    "loss_name =  \"cross_entropy_weighted\"\n",
    "building_class_weight = 1\n",
    "label_smoothing = 0.0\n",
    "lr = 0.00005\n",
    "momentum = float\n",
    "scheduler_name = \"one_cycle\"\n",
    "scheduler_patience = 3\n",
    "patience = 200\n",
    "from_s3 = 0\n",
    "seed = 12345 \n",
    "cuda = 0\n",
    "cuda = cuda and torch.cuda.is_available()\n",
    "kwargs = {\"num_workers\": os.cpu_count(), \"pin_memory\": True} if cuda else {}\n",
    "\n",
    "dep, year  = \"MARTINIQUE\", \"2022\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0019.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0019.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0028.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0028.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0512_8592_U38S_8Bits_0005.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0512_8592_U38S_8Bits_0005.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0034.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0034.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0005.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0005.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0029.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0029.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0026.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0026.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0039.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0039.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8594_U38S_8Bits_0005.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8594_U38S_8Bits_0005.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8595_U38S_8Bits_0003.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8595_U38S_8Bits_0003.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0025.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0025.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0031.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0031.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0011.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0011.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0013.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0013.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0040.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0040.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0006.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0006.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0041.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0041.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0003.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0003.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0043.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0043.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0000.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0000.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0044.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0044.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0045.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0045.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0047.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0047.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0004.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0004.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0049.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0049.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0009.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0009.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0007.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0007.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0006.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0006.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0018.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0018.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0012.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0012.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0014.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0014.jp2`\n",
      "`s3/projet-slums-detection/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0015.jp2` -> `data/data-preprocessed/golden-test/patchs/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0015.jp2`\n",
      "Total: 2.36 MiB, Transferred: 2.36 MiB, Speed: 6.11 MiB/s\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0029.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0029.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0512_8592_U38S_8Bits_0005.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0512_8592_U38S_8Bits_0005.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0025.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0025.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8595_U38S_8Bits_0003.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8595_U38S_8Bits_0003.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0034.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0034.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0005.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0005.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0041.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0041.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0043.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0043.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0031.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8593_U38S_8Bits_0031.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0026.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0513_8568_U38S_8Bits_0026.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8594_U38S_8Bits_0005.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0514_8594_U38S_8Bits_0005.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0003.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0003.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0019.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0019.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0006.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0006.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0011.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0011.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0013.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0013.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0040.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0040.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0028.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0028.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0039.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0039.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0044.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0044.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0049.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0049.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0045.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0045.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0047.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0524_8587_U38S_8Bits_0047.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0014.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0014.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0000.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0000.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0007.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8587_U38S_8Bits_0007.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0009.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0009.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0004.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0004.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0006.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0006.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0018.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0018.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0012.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0012.npy`\n",
      "`s3/projet-slums-detection/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0015.npy` -> `data/data-preprocessed/golden-test/labels/segmentation/PLEIADES/MAYOTTE_CLEAN/2022/250/ORT_976_2022_0525_8588_U38S_8Bits_0015.npy`\n",
      "Total: 1.91 MiB, Transferred: 1.91 MiB, Speed: 2.60 MiB/s\n"
     ]
    }
   ],
   "source": [
    "patches, labels = get_patchs_labels(\n",
    "        from_s3, task, source, dep, year, tiles_size, type_labeler, train=True\n",
    "    )\n",
    "\n",
    "train_patches = []\n",
    "train_labels = []\n",
    "test_patches = []\n",
    "test_labels = []\n",
    "normalization_means = []\n",
    "normalization_stds = []\n",
    "weights = []\n",
    "\n",
    "patches.sort()\n",
    "labels.sort()\n",
    "indices = filter_indices_from_labels(labels, -1.0, 2.0)\n",
    "train_patches += [patches[idx] for idx in indices]\n",
    "train_labels += [labels[idx] for idx in indices]\n",
    "\n",
    "module_name\n",
    "\n",
    "\n",
    "patches, labels = get_patchs_labels(\n",
    "    from_s3, task, source, dep, year, tiles_size, type_labeler, train=False\n",
    ")\n",
    "\n",
    "patches.sort()\n",
    "labels.sort()\n",
    "test_patches += list(patches)\n",
    "test_labels += list(labels)\n",
    "\n",
    "normalization_mean, normalization_std = normalization_params(\n",
    "    task, source, dep, year, tiles_size, type_labeler\n",
    ")\n",
    "normalization_means.append(normalization_mean)\n",
    "normalization_stds.append(normalization_std)\n",
    "weights.append(len(indices))\n",
    "\n",
    "# Golden test\n",
    "golden_patches, golden_labels = get_golden_paths(\n",
    "    from_s3, task, source, \"MAYOTTE_CLEAN\", \"2022\", tiles_size\n",
    ")\n",
    "\n",
    "golden_patches.sort()\n",
    "golden_labels.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalization_mean = np.average(\n",
    "    [mean[:n_bands] for mean in normalization_means], weights=weights, axis=0\n",
    ")\n",
    "normalization_std = [\n",
    "    pooled_std_dev(\n",
    "        weights,\n",
    "        [mean[i] for mean in normalization_means],\n",
    "        [std[i] for std in normalization_stds],\n",
    "    )\n",
    "    for i in range(n_bands)\n",
    "]\n",
    "\n",
    "transform_list = [\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.Normalize(\n",
    "        max_pixel_value=1.0,\n",
    "        mean=normalization_mean,\n",
    "        std=normalization_std,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "]\n",
    "\n",
    "if augment_size != tiles_size:\n",
    "    transform_list.insert(0, A.Resize(augment_size, augment_size))\n",
    "transform = A.Compose(transform_list)\n",
    "\n",
    "test_transform_list = [\n",
    "    A.Normalize(\n",
    "        max_pixel_value=1.0,\n",
    "        mean=normalization_mean,\n",
    "        std=normalization_std,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "]\n",
    "if augment_size != tiles_size:\n",
    "    test_transform_list.insert(0, A.Resize(augment_size, augment_size))\n",
    "test_transform = A.Compose(test_transform_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(task, train_patches, train_labels, n_bands, from_s3, transform)\n",
    "dataset = get_dataset(task, train_patches[:40], train_labels[:40], n_bands, from_s3, transform)\n",
    "test_dataset = get_dataset(task, test_patches, test_labels, n_bands, from_s3, test_transform)\n",
    "golden_dataset = get_dataset(\n",
    "    task, golden_patches, golden_labels, n_bands, from_s3, test_transform\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=Generator())\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=test_batch_size, shuffle=False, drop_last=True, **kwargs\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, shuffle=False, drop_last=True, **kwargs\n",
    ")\n",
    "golden_loader = DataLoader(\n",
    "    golden_dataset, batch_size=test_batch_size, shuffle=False, drop_last=True, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/mamba/lib/python3.11/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage output shape: torch.Size([8, 512, 8, 8])\n",
      "Stage output shape: torch.Size([8, 512, 8, 8])\n",
      "Stage output shape: torch.Size([8, 512, 8, 8])\n",
      "Stage output shape: torch.Size([8, 512, 8, 8])\n",
      "Concatenated shape: torch.Size([8, 4096, 8, 8])\n",
      "tensor([[[[ 0.3415,  0.3415,  0.3415,  ...,  0.1319,  0.1319,  0.1319],\n",
      "          [ 0.3415,  0.3415,  0.3415,  ...,  0.1319,  0.1319,  0.1319],\n",
      "          [ 0.3415,  0.3415,  0.3415,  ...,  0.1319,  0.1319,  0.1319],\n",
      "          ...,\n",
      "          [ 0.4882,  0.4882,  0.4882,  ..., -0.0425, -0.0425, -0.0425],\n",
      "          [ 0.4882,  0.4882,  0.4882,  ..., -0.0425, -0.0425, -0.0425],\n",
      "          [ 0.4882,  0.4882,  0.4882,  ..., -0.0425, -0.0425, -0.0425]]],\n",
      "\n",
      "\n",
      "        [[[-0.1728, -0.1728, -0.1728,  ..., -0.1293, -0.1293, -0.1293],\n",
      "          [-0.1728, -0.1728, -0.1728,  ..., -0.1293, -0.1293, -0.1293],\n",
      "          [-0.1728, -0.1728, -0.1728,  ..., -0.1293, -0.1293, -0.1293],\n",
      "          ...,\n",
      "          [-0.5636, -0.5636, -0.5636,  ...,  0.2874,  0.2874,  0.2874],\n",
      "          [-0.5636, -0.5636, -0.5636,  ...,  0.2874,  0.2874,  0.2874],\n",
      "          [-0.5636, -0.5636, -0.5636,  ...,  0.2874,  0.2874,  0.2874]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2407,  0.2407,  0.2407,  ..., -0.5404, -0.5404, -0.5404],\n",
      "          [ 0.2407,  0.2407,  0.2407,  ..., -0.5404, -0.5404, -0.5404],\n",
      "          [ 0.2407,  0.2407,  0.2407,  ..., -0.5404, -0.5404, -0.5404],\n",
      "          ...,\n",
      "          [-0.0217, -0.0217, -0.0217,  ...,  0.3889,  0.3889,  0.3889],\n",
      "          [-0.0217, -0.0217, -0.0217,  ...,  0.3889,  0.3889,  0.3889],\n",
      "          [-0.0217, -0.0217, -0.0217,  ...,  0.3889,  0.3889,  0.3889]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1307, -0.1307, -0.1307,  ..., -0.0545, -0.0545, -0.0545],\n",
      "          [-0.1307, -0.1307, -0.1307,  ..., -0.0545, -0.0545, -0.0545],\n",
      "          [-0.1307, -0.1307, -0.1307,  ..., -0.0545, -0.0545, -0.0545],\n",
      "          ...,\n",
      "          [ 0.6334,  0.6334,  0.6334,  ...,  0.0834,  0.0834,  0.0834],\n",
      "          [ 0.6334,  0.6334,  0.6334,  ...,  0.0834,  0.0834,  0.0834],\n",
      "          [ 0.6334,  0.6334,  0.6334,  ...,  0.0834,  0.0834,  0.0834]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0879,  0.0879,  0.0879,  ..., -0.1488, -0.1488, -0.1488],\n",
      "          [ 0.0879,  0.0879,  0.0879,  ..., -0.1488, -0.1488, -0.1488],\n",
      "          [ 0.0879,  0.0879,  0.0879,  ..., -0.1488, -0.1488, -0.1488],\n",
      "          ...,\n",
      "          [ 0.4001,  0.4001,  0.4001,  ...,  0.3770,  0.3770,  0.3770],\n",
      "          [ 0.4001,  0.4001,  0.4001,  ...,  0.3770,  0.3770,  0.3770],\n",
      "          [ 0.4001,  0.4001,  0.4001,  ...,  0.3770,  0.3770,  0.3770]]],\n",
      "\n",
      "\n",
      "        [[[-0.1356, -0.1356, -0.1356,  ...,  0.2717,  0.2717,  0.2717],\n",
      "          [-0.1356, -0.1356, -0.1356,  ...,  0.2717,  0.2717,  0.2717],\n",
      "          [-0.1356, -0.1356, -0.1356,  ...,  0.2717,  0.2717,  0.2717],\n",
      "          ...,\n",
      "          [ 0.4632,  0.4632,  0.4632,  ...,  0.0550,  0.0550,  0.0550],\n",
      "          [ 0.4632,  0.4632,  0.4632,  ...,  0.0550,  0.0550,  0.0550],\n",
      "          [ 0.4632,  0.4632,  0.4632,  ...,  0.0550,  0.0550,  0.0550]]]],\n",
      "       grad_fn=<UpsampleBilinear2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = get_model(module_name,3, True, False)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "labels = batch[\"labels\"]\n",
    "images = batch[\"pixel_values\"]\n",
    "\n",
    "output = model(images)\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
