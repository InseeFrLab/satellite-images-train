{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_from_id(run_id: str) -> mlflow.pyfunc.PyFuncModel:\n",
    "    \"\"\"\n",
    "    This function fetches a trained machine learning model from the MLflow\n",
    "    model registry based on the specified model name and version.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to fetch from the model\n",
    "        registry.\n",
    "        model_version (str): The version of the model to fetch from the model\n",
    "        registry.\n",
    "    Returns:\n",
    "        model (mlflow.pyfunc.PyFuncModel): The loaded machine learning model.\n",
    "    Raises:\n",
    "        Exception: If the model fetching fails, an exception is raised with an\n",
    "        error message.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        model = mlflow.pyfunc.load_model(model_uri=f\"runs:/{run_id}/model\")\n",
    "        return model\n",
    "    except Exception as error:\n",
    "        raise Exception(f\"Failed to fetch model from run_id : {run_id}\") from error\n",
    "\n",
    "\n",
    "def fetch_model(run_id):\n",
    "    # Load the ML model\n",
    "    model = get_model_from_id(run_id)\n",
    "\n",
    "    # Extract several variables from model metadata\n",
    "    n_bands = int(mlflow.get_run(model.metadata.run_id).data.params[\"n_bands\"])\n",
    "    tiles_size = int(mlflow.get_run(model.metadata.run_id).data.params[\"tiles_size\"])\n",
    "    augment_size = int(mlflow.get_run(model.metadata.run_id).data.params[\"augment_size\"])\n",
    "    module_name = mlflow.get_run(model.metadata.run_id).data.params[\"module_name\"]\n",
    "    normalization_mean, normalization_std = get_normalization_metrics(model, n_bands)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"n_bands\": n_bands,\n",
    "        \"tiles_size\": tiles_size,\n",
    "        \"augment_size\": augment_size,\n",
    "        \"normalization_mean\": normalization_mean,\n",
    "        \"normalization_std\": normalization_std,\n",
    "        \"module_name\": module_name,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_normalization_metrics(model: mlflow.pyfunc.PyFuncModel, n_bands: int):\n",
    "    \"\"\"\n",
    "    Retrieves normalization metrics (mean and standard deviation) for the model.\n",
    "\n",
    "    Args:\n",
    "        model (mlflow.pyfunc.PyFuncModel): MLflow PyFuncModel object representing the model.\n",
    "        n_bands (int): Number of bands in the satellite image.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing normalization mean and standard deviation.\n",
    "    \"\"\"\n",
    "    normalization_mean = json.loads(\n",
    "        mlflow.get_run(model.metadata.run_id).data.params[\"normalization_mean\"]\n",
    "    )\n",
    "    normalization_std = json.loads(\n",
    "        mlflow.get_run(model.metadata.run_id).data.params[\"normalization_std\"]\n",
    "    )\n",
    "\n",
    "    # Extract normalization mean and standard deviation for the number of bands\n",
    "    normalization_mean, normalization_std = (\n",
    "        normalization_mean[:n_bands],\n",
    "        normalization_std[:n_bands],\n",
    "    )\n",
    "\n",
    "    return (normalization_mean, normalization_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"5cf1eb7bdd4141529688acdd738e739e\"\n",
    "model_info = fetch_model(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_info[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "import argparse\n",
    "import gc\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "\n",
    "import albumentations as A\n",
    "import mlflow\n",
    "import torch\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from osgeo import gdal\n",
    "from torch import Generator\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from functions.download_data import (\n",
    "    get_file_system,\n",
    "    get_patchs_labels,\n",
    "    normalization_params,\n",
    "    get_golden_paths,\n",
    "    pooled_std_dev,\n",
    ")\n",
    "from functions.instanciators import get_dataset, get_lightning_module, get_trainer\n",
    "from functions.filter import filter_indices_from_labels\n",
    "\n",
    "gdal.UseExceptions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_server_uri = \"https://projet-slums-detection-128833.user.lab.sspcloud.fr\"\n",
    "experiment_name = \"test-dev\"\n",
    "run_name =  \"stagiosessaye\"\n",
    "task = \"segmentation\"\n",
    "source = \"PLEIADES\"\n",
    "tiles_size = 250\n",
    "augment_size = 250\n",
    "type_labeler = \"BDTOPO\"\n",
    "n_bands = 3\n",
    "logits = 1\n",
    "freeze_encoder = 0\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "test_batch_size = 8\n",
    "num_sanity_val_steps = 1\n",
    "accumulate_batch = 8\n",
    "module_name = \"deeplab-v3plus-ocr\"\n",
    "loss_name =  \"cross_entropy_weighted\"\n",
    "building_class_weight = 1\n",
    "label_smoothing = 0.0\n",
    "lr = 0.00005\n",
    "momentum = float\n",
    "scheduler_name = \"one_cycle\"\n",
    "scheduler_patience = 3\n",
    "patience = 200\n",
    "from_s3 = 0\n",
    "seed = 12345 \n",
    "cuda = 0\n",
    "cuda = cuda and torch.cuda.is_available()\n",
    "kwargs = {\"num_workers\": os.cpu_count(), \"pin_memory\": True} if cuda else {}\n",
    "\n",
    "deps = [\"MAYOTTE\",\"MAYOTTE\",\"MAYOTTE\"]\n",
    "years = [\"2017\",\"2019\",\"2020\"]\n",
    "\n",
    "normalization_means = []\n",
    "normalization_stds = []\n",
    "\n",
    "for dep,year in zip(deps,years):\n",
    "    normalization_mean, normalization_std = normalization_params(\n",
    "        task, source, dep, year, tiles_size, type_labeler\n",
    "    )\n",
    "    normalization_means.append(normalization_mean)\n",
    "    normalization_stds.append(normalization_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Golden test\n",
    "golden_patches, golden_labels = get_golden_paths(\n",
    "    from_s3, task, source, \"MAYOTTE_CLEAN\", \"2022\", tiles_size\n",
    ")\n",
    "\n",
    "golden_patches.sort()\n",
    "golden_labels.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Assuming normalization_means, normalization_stds, and n_bands are defined elsewhere\n",
    "normalization_mean = np.average(\n",
    "    [mean[:n_bands] for mean in normalization_means], weights=[1.0, 1.0, 1.0], axis=0\n",
    ")\n",
    "\n",
    "normalization_std = [\n",
    "    pooled_std_dev(\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [mean[i] for mean in normalization_means],\n",
    "        [std[i] for std in normalization_stds],\n",
    "    )\n",
    "    for i in range(n_bands)\n",
    "]\n",
    "\n",
    "# Convert numpy arrays to lists\n",
    "normalization_mean_list = normalization_mean.tolist()\n",
    "\n",
    "\n",
    "test_transform = [\n",
    "    A.Normalize(\n",
    "        max_pixel_value=1.0,\n",
    "        mean=normalization_mean_list,\n",
    "        std=normalization_std,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset = get_dataset(\n",
    "    task, golden_patches, golden_labels, n_bands, from_s3, test_transform\n",
    ")\n",
    "\n",
    "golden_loader = DataLoader(\n",
    "    golden_dataset, batch_size=test_batch_size, shuffle=False, drop_last=True, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(golden_loader))\n",
    "labels = batch[\"labels\"]\n",
    "images = batch[\"pixel_values\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s atellites_images inference\n",
    "    # Preprocess the image\n",
    "    normalized_si = preprocess_image(\n",
    "        model=model,\n",
    "        image=image,\n",
    "        tiles_size=tiles_size,\n",
    "        augment_size=augment_size,\n",
    "        n_bands=n_bands,\n",
    "        normalization_mean=normalization_mean,\n",
    "        normalization_std=normalization_std,\n",
    "    )\n",
    "\n",
    "    # Make prediction using the model\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.tensor(model.predict(normalized_si.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
